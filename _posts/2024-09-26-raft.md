---
date: 2024-09-11
title: "MIT-6.5840-Raft"
category: 学习经验
tags: [MIT-6.5840,分布式系统]
excerpt: "Raft论文解读以及lab3实现思路"
---


# Raft论文解读

从中央集权（单master）到自由民主的伟大过渡！（虽然选举后仍然是中央集权，而且集权程度比GFS还严重😂😂😂

TODO!!!

# Lab3实现思路

Raft虽然宣称是一个**understandable**的算法，论文阅读起来idea确实很简单，无非就是投票嘛，但实际实现起来却要面对无限的细节问题，实现起来极其麻烦，再次验证了*Talk is cheap*的不变真理

## Part 3A: leader election

虽然只是一个刚刚开始的部分，难度评级也只有moderate，但个人前后花费了将近三天才完成这一部分。最后的结果是成功连续进行了200多次实验才失败，考虑到raft选举的随机性，应该是可以接受的结果。实现的代码不多，大概实际编写的只有两三百行左右，下面主要说说实现思路和一些踩到的坑

### 准备工作

**论文的Figure 2非常的重要！！！**

**论文的Figure 2非常的重要！！！**

**论文的Figure 2非常的重要！！！**

![](/assets/images/2024-09-26-raft/Figure1.png)

在开始前一定要将每一条行为都看一遍，在实现时也要不断地对照这张图的语义来实现，在本部分所有踩到的坑大部分都是因为没有仔细看这张图导致的。。。

### 超时触发器

raft的核心就在于心跳包超时后开启的选举，所以第一步我选择先实现超时的处理


框架代码给我们准备了基础的逻辑，即周期地判断服务器是否死亡，如果没有死亡就进行超时检查

在服务器的数据结构中记录好上一次的心跳时间戳，命名为`lastHeartBeat`，在判断时使用`time.Since(rf.lastHeartBeat) > TM_ElectionTimeout`即可判断是否完成检查

接着根据论文中提到的随机化策略，进行随机的时间暂停，随后启动选举

逻辑上就是这么简单，但是有几个细节需要注意

1. 
    随机的时间暂停后，应该再次判断心跳时间是否被刷新，角色是否被更改。进行这些判断保证开启选举时服务器一定处于超时并且角色是Candidate，避免没有必要的选举

2. 
    选举应该要在一个新的线程中启动，然后刷新心跳时间，回到主循环等待下一次超时或者成为Leader

    在最开始的实现中，每一个选举都在主线程中开启，选举失败超时由另外的计时器来决定。这样做让参与当前选举的Candidate无法参与到下一轮的选举中，而理论上所有的Candidate参与的机会是均等的

下面是实现的代码（虽然天天说[学术诚信](https://integrity.mit.edu/)，但是果然还是代码的说明更加方便）：
```go
func (rf *Raft) ticker() {
	for !rf.killed() {
		// Check if a leader election should be started.
		rf.mu.Lock()

		// Heart beat timeout
		if rf.state != RS_Leader && time.Since(rf.lastHeartBeat) > TM_ElectionTimeout {
			log.Printf("[%v] heart beat timeout\n", rf.me)
			rf.state = RS_Candiate
			log.Printf("[%v] turn to candiate\n", rf.me)
			// randomization
			rf.mu.Unlock()
			ms := (rand.Int63() % int64(TM_RandomWaitingTime))
			time.Sleep(time.Duration(ms))
			rf.mu.Lock()

			// Start an election, if other one win the election, state will not be Candiate
			// or voting to someone that reset heartbeat time
			if rf.state != RS_Candiate || time.Since(rf.lastHeartBeat) < TM_ElectionTimeout {
				rf.mu.Unlock()
			} else {
				rf.lastHeartBeat = time.Now() // reset election timer
				rf.mu.Unlock()
				go rf.election()
			}
		} else {
			rf.mu.Unlock()
		}

		// pause for a random amount of time between 50 and 350
		// milliseconds.
		ms := 50 //+ (rand.Int63() % 300)
		time.Sleep(time.Duration(ms) * time.Millisecond)
	}
}
```

### 选举

超时后就到了紧张刺激的选举时间

#### 逻辑实现

按照论文Figure 2的逻辑，Candidate在开启选举时要做的是

1. 增加自身的任期号

2. 将票投给自己

3. 重置选举超时计时器

4. 向所有的服务器发起RequestVote RPC

增加自身任期号是为了开启一个新的选举任期，在刚开始的新任期中，该候选人一定还没有进行过投票，所以可以保证将票投给自己

重置超时计时器我将其放到了开启选举线程之后的主线程行为中

接着向所有的服务器并行地发起RequestVote RPC，要注意RPC是会随机失败的，所以根据论文我们要`indefinitely`发送RPC，但是肯定不能真的无限地发送，当这次选举失效后要主动停止，停止的机制后续介绍

当受到肯定的回复后，递增选举票数计数器

在启动了所有的发送RPC的线程后，在选举主线程上进行轮询等待票数超过多数或该次选举失败

#### 该次选举失败

在实现中，最让人头疼的就是如何处理失败的选举

失败可以由以下几个原因导致

1. 选举超时，开启了下一轮选举，该轮选举失效

2. 收到了来自相同或更高任期的心跳包，说明该轮或者更高轮已出现胜者

3. 发送的RequestVote RPC被一个拥有更高任期号的机器回复，说明当前选举周期已经失效

1，2都是和当前选举活动无关的，它们由其他线程的行为触发，3是在处理了RequestVote RPC的返回值后发生的

但好在它们都会导致一样的影响：任期号更新到更高的值或者角色变化为Follower，也可能两者都发生

所以如果要终止无限制的RPC请求或者选举主线程的等待，只需要关注任期号或者角色的变化即可

如果任期号不再和开始选举时一致，说明失败

如果角色不再是Candidate，也说明失败

**下面是并行发起RPC的代码：**

只有在确定了`isCandiate`和`isSameTerm`的情况下才会发起RPC，否则直接退出线程

当RPC的收到一个来自更高任期的回复时，将任期号更新到回复相同，重置状态，等待该候选人发送RPC时给它投票

注意这里有一个**细节**，重置状态在所有并行的线程中应该只能发生一次，如果因为时序问题多次重置状态可能就会出现在同一任期将票投给多个人的情况

```go
votedForTerm := rf.currentTerm
votedForCandiate := rf.me
isCancel := false

for i := range rf.peers {
    if i == rf.me {
        continue
    }

    go func(num int) {
        reply := RequestVoteReply{}

        // repeat sending vote request
        ok := false
        for !ok {
            rf.mu.Lock()
            isCandiate := rf.state == RS_Candiate
            isSameTerm := rf.currentTerm == votedForTerm
            rf.mu.Unlock()
            if isCandiate && isSameTerm {
                ok = rf.sendRequestVote(num, &args, &reply)
            } else {
                return
            }
        }
        // log.Printf("[%v] send RequestVote to [%v] successfully\n", votedForCandiate, num)

        if !reply.VoteGranted && reply.Term > votedForTerm { // if there is a higher term, stop election
            rf.mu.Lock()

            // only stop once
            if !isCancel {
                rf.currentTerm = reply.Term
                rf.state = RS_Follower
                rf.votedFor = nil
                log.Printf("[%v] stop election because [%v] has higher term, turn to follower\n", votedForCandiate, num)
                isCancel = true
            }

            rf.mu.Unlock()
        } else if reply.VoteGranted { // granted!
            ballotMutex.Lock()
            ballotCount += 1
            ballotMutex.Unlock()
            log.Printf("[%v] get ballot from [%v]", votedForCandiate, num)
        }
    }(i)
}
```

选举主线程做的判断也类似，关注任期号的变化或者角色的变化，此处不再列举

#### 赢得选举

当得到的投票超过一半（`ballot >= len(rf.peers)/2+1`，包括自身的那一票)，并且当前确实处于Candidate的角色，则该候选人成为Leader

接着开启发送心跳包的线程，然后结束选举线程

发起心跳包的线程就是不断发起`AppendEntries`，但是`Entries`字段为空，其所做的结束判断和选举中所做的基本一致，不再赘述

### 两种RPC的行为

Raft为了简化设计，增强自身的可理解性，只设计了两种RPC即可完成所有的任务

将心跳包合并到添加条目RPC确实是一个聪明的做法

#### AppendEntries

还是那句话，一定要仔细阅读论文Figure2的说明，所有的操作都已经被定义好了

当接受到来自任期更低的RPC请求，接受者选择拒绝该RPC，然后将自己的任期号返回给发送者。

发送者接受到回复后如果发现发现自己的任期号低于一个现有的机器，则放弃Leader的权限，转变为follower，提升任期与回复中的相同

如果该RPC的发送方任期大于或等于接受者，无论现在的状态怎么样，都转变为Follower，同时同步任期号（如果此时接受者正处于选举状态，则会导致了前面提到的选举失败）

然后接受者重置心跳计时器，避免超时

在这个阶段的AppendEntries RPC还是比较简单的，只承担心跳的作用，但是到了后面真正实现日志的时候就需要大改特改这个设计了

#### RequestVote

在当前阶段的RequestVote中，因为日志没有任何的添加，所以也没有逻辑判断日志的高低与否

一个接受者根据接受到的RequestVote中的任期信息，作出以下反应

1. **发送者的任期号小于接受者**

    这个情况下和AppendEnries是一样的，接受者拒绝该请求，同时将自己的任期返回回去，发送者就可以根据返回的任期来判断自己是否要继续担任Leader

2. **发送者的任期号大于接受者**

    此处的大于是**严格大于**，当接受者发现自己的任期落后于发送者，则提高自己的任期号和发送者相同，将`votedFor`置为null，转变为Follower，等待进行下一步的判定

3. **发送者的任期号等于接受者**

    如果任期号相同， 则判定该任期内是否已经将票投给了某人（通过判定`votedFor`是否为null）。如果没有则修改`votedFor`为发送者的Id，回复肯定的消息给发送者。

注意以上的状态不是使用if else而是顺序地判定的，也就是说触发了条件2的接受者可以参与条件3的判定。所以一个任期号落后的机器如果收到了来自高任期的RequestVote就会马上将票投给该发送者，因为条件2会把`votedFor`置为null

### votedFor

到此Part 3A从启动开始到选举后发送heart beat的机制按照执行的先后顺序讲完了，下面说说在实现中，一直困扰我的一个概念：`votedFor`

非常遗憾论文中并没有怎么详细提及这一属性到底要如何变化，有限的讨论如下：

> **votedFor**: candidateId that received vote in current term (or null if none)

> If **votedFor** is null or candidateId, and candidate’s log is at
least as up-to-date as receiver’s log, grant vote

第一条提到`votedFor`是用来追踪当前任期该机器将票投给了谁。在最开始我一直不理解单一的变量要怎么追踪多个任期下怎么投票的，还想着要不要建立一个数组来追踪所有任期下的投票情况

但是后续我才明白为什么只需要一个数组就可以维护所有任期的投票情况。因为任期`term`和`votedFor`是严格绑定的。只要任期号发生变化，`votedFor`一定也跟着变化（投给自己或者变为nil）

让我们来细数下`votedFor`在目前的实现中会发生变化的情况

1. 启动时`votedFor`被初始化为null

1. 在接收RequestVote RPC时，接受者任期号低于发送者任期号。接受者将任期号更新至发送者的任期号，同时将`votedFor`置为null

2. RequestVote RPC中，接受者将票投给了发送者，将`votedFor`更新为发送者的Id，此时发送者和接受者任期号一定相同

3. 一位候选人发起了一轮选举，它自增自己的任期号，然后将`votedFor`设置为自己的Id号

4. 候选人开启选举后，向其他机器发起RequestVote时，受到一位任期比它高的机器的拒绝回复。该候选人将任期号更新至回复者，变为Follower，然后将`votedFor`设置为null

5. 一位Leader在发送心跳包时，受到了一位任期号比它大的机器的回复。该候选人将任期号更新至回复者，变为Follower，然后将`votedFor`设置为null

可以看到，两者的变化是严格绑定的。任期号的变化一定会使`votedFor`发生变化。而`votedFor`在一个任期内要发生更改的前提是它为null即还没有投票给任何人，从此保证了在一个周期内不会出现多投票的问题

不过有一说一，论文中关于`votedFor`的另外一条描述让人费解，似乎是因为论文的做法是不断地发送RequestVote RPC直到超过半数的机器回复过肯定，该候选人成为Leader为止。而我的做法是只进行一次成功的RPC，如果成功递增计数器，靠计数器来判断是否超过了半数。

所以论文中说到RequestVote的接受者如果`votedFor`和`CandiateId`一致也返回肯定的回复

但这样的做法感觉除了浪费了网络资源以外就没有任何意义了（不知道是不是我理解错了）

Anyway，至少在目前的机制下，`votedFor`很好地解决了重复投票的问题


## Part 3B: log

评级给到了hard，确实很难。国庆期间拼拼凑凑搞了四天才搞定。每天大概花了四五个小时在这上面

理念都很简单，但是实现起来就要面对无限的细节，fuck system！

### 复盘

在介绍前先自我复盘一下实现过程的一些问题，做一下总结

1. **遵循paper的指导**

    事后看来，大部分问题的根源都是没有仔细阅读论文的Figure 2。我要做的是按照paper的指导实现一个系统，而不是要创新一个新系统，毕竟当前的我没有必要也没有能力做到创新。所以下个部分一定要复习了论文再做！

2. **拒绝线性思维**

    在最开始的实现中，没有深刻地理解事件驱动编程的强大之处，使用了很多线性思想。
    
    比如在`Raft.Start`中，最开始的实现是在leader为本地的log添加entry时立刻启动对follower的追加。这样做的后果是实现起来非常复杂，在一个函数中既要处理添加同时还要处理多个发起的协程的中止处理等等，总之就是为了同步状态要花费非常多的精力。

    正确的实现是在`Raft.Start`中，leader添加完本地entry之后立刻返回，然后由另外一个单独的协程完成同步任务。这个单独的协程名为`Raft.syncEntries`，是part B最为重要的一部分，它定期同步leader和follower的log状态，同时充当原本的heart beat功能。当它检查到leader的下标超过follower时自然会启动对follower的追加，然后定期检查是否有满足commit条件的条目，更新leader的`commitIndex`。再有一个单独的协程检查`commitIndex`和`lastApplied`的关系，自动处理apply。

    整个系统处于事件驱动中，由多个协程自动检查事件是否发生：选举超时，log增加，commitIndex增加等等。不得不说事件驱动编程真是伟大的发明

3. **拒绝过早优化**

    > 过早优化是万恶之源

    事实如此，在实现过程中，至少花了三四个小时在无意义的过早优化中。

    比如在实现中我发现Raft中的有些状态比如说当前的角色，当前的任期等在很多事件检查中都是只读的，很自然地就想到模仿框架中的`Raft.killed`函数对这些状态做一下原子化的检查和更改。

    第一个遇到的问题是go的原子操作只支持`int32`,`int64`这样严格指定了字长的类型，而我在早期编写时图方便用了`int`。而go中`int`偏偏不是`int32`的简单别名。这就导致go的原子操作包拒绝接受以`int`为类型的对象，而我又不想引入unsafe包来强制转换，所以只能把所有的`int`都修改为了`int32`

    第二个遇到的问题是有的数据在代码中被引用了太多次，要修改全部太过困难，特别是当前的任期，原来的选举系统大量地引用这个变量。虽然ide的引用查询很好用，但仍是一个艰难的过程

    然而在处理了这么多问题之后，在开启race检查后仍然出现了race condition。在多次尝试处理未果后还是放弃了原子化的检查和更改

    虽然在原子化的检查和更改上花费了大量时间，但事实表明这是无用功。在实现一个系统时，应该把实现功能放在第一位，而且MIT的教授也多次说了lab的测试不看重效率。而且在编写的时候一直在思考要怎么优雅地写go，后面想了想我都写go了还要什么优雅，就算代码长得跟HTML一样又怎么样，能跑就行！

    总之要拒绝过早优化

4. **编程基础还是要牢固**

    在debug的过程中，发现了几处错误是由于不牢固的基础导致的

    比如在git commit记录中可以看到修了好几次index out of range错误，这是因为在使用下标访问数组（切片）时没有下意识地检查下标的合法性导致的，这个错误显然是能够通过良好的编程习惯解决的

    还有一个问题就是深浅拷贝带来的困扰。在创建一个AppendEntriesArgs的时候，为`Entries`赋值时使用简单的`rf.log[rf.nextIndex[i]:]`创建的是对log的浅拷贝。因为在发送RPC的时候出于效率考虑是没有进行锁保护的，这时如果有某个协程更改了log，就会导致数据竞争。正确的做法是完整地为AppendEntriesArgs的`Entries`参数复制一份完整的深拷贝，比如使用`append([]logEntry(nil), rf.log[rf.nextIndex[i]:]...)`

    只对所有能想到的情况进行处理，对没有预期的情况抛出一个`no implementation`错误，这在开发的时候定位问题是十分重要的

大概就以上几点吧，这个part确实难啊，但是一想到只要实现了这个part至少现在的Raft确实能容错了（虽然只容忍网络错误，不能容忍崩溃错误）