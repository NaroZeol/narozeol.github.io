---
date: 2024-11-26
title: "MIT-6.5840-KVRaft"
category: 学习经验
tags: [MIT-6.5840,分布式系统]
excerpt: "MIT-6.5840 Lab 4 实现回顾"
---

额，看了下上次更新文章居然已经是一个月之前了。

Emm。。。首先这个月确实有点摆，感觉在Raft上有些吹毛求疵，花了将近一个月的时间去处理一些潜在的bug（然而lab4还是暴露出了一大堆）。还有就是随着学期进入期中，多了一些难受的实验课（一学分实验让我抄了50页报告），空闲时间没有之前多了。进入冬季日常犯困的情况也多了起来。总之这个月确实进度非常缓慢，只完成了lab4。按这个进度下去12月中旬应该就可以完成全部lab了，到时候考虑下是进一步学习还是尝试下导师给的课题吧。

总的来说lab4并不困难，只要你的raft的实现的足够好的话，但非常可惜我的raft存在很多问题，大部分时间都花在和raft.go搏斗上了。

# 对原先Raft的不足之处的修改

这部分反省下原先的Raft的实现的不足以及解决方案

## 在Leader当选时立刻添加一条空日志

首先明确一点，Leader应该只commit当前任期内的日志，如果不保证这点就会出现论文中Figure 8的不一致状态。

然后考虑这样一个情况：
```
S1:     1 1 1 2 2 
S2:     1 1 1 2
S3:     1 1 1 2 2
```

上面的数字代表日志的任期号，假设S1为Leader，任期为2，它添加了两条日志，并且都达成了commit条件。然后此时S1崩溃，重新恢复后，假设从S3中选举出，任期为3。

这时根据我们的基本设定，Leader只commit它任期内的日志，S3没有权力commit任期2的第二条日志。这时如果上层应用不调用Start()来创建一条新的任期3的日志，系统就会进入阻塞状态。

这促使我不得不实现了空日志机制，然后按照论文的描述，在一个Leader的成功赢得选举后，立刻向日志中加入一条空日志来启动同步，因为空日志的任期就等于当前任期，所以Leader可以commit它，同时也就commit之前没有commit的日志。

空日志机制听起来简单，但是实际比想象中困难。最大的问题是lab3的测试默认raft和上层应用交流时使用的下标是从1开始连续的。所以我们不能直接将raft中的下标直接作为applyMsg的下标，因为存在空日志会让下标不连续。

所以我不得不又引入了一个新的中间转换层，将Raft和上层应用之间交流的下标称为externalIndex，而Raft内部的下标称为internalIndex。每次要交流下标时都要做一次转换，转换方式我只想到了一个笨方法，遍历当前本地的日志，剔除空日志。虽然听起来很笨但实现起来也很麻烦，因为要考虑snapshot机制，为此我还无法避免地去修改了InstallSnapshot RPC的argument。

虽然这个问题在后续更换了KV-server的实现后就自然而然地消失了（最开始的实现server在重试时会等待某个特定Index的日志commit，这种等待导致了系统的阻塞，后面实现中每一次重试都创建一条新的日志，就自然不会遇到这种问题了），但还是作为一个重要机制保留下来了，毕竟这是论文中也有提及的做法。

## 使用无锁队列发送applyMsg

大概是因为实现的不同，我的kvserver相比lab3的测试框架代码更容易触发带锁apply时可能的死锁。具体就是在实现InstallSnapshot时，为了保证snapshot万无一失地发送到了上层应用然后再开始apply日志条目，我选择带锁进行apply，并且错误地认为这样做不会造成死锁问题（额，至少在lab3的测试中这个问题几乎没有发生过）

具体的出发流程是这样的：（在修改后的lab3回顾中也有提及，这里直接照搬了）

1. Raft收到InstallSnapshot RPC, 获取的`rf.mu`锁，随后向管道发送snapshot（未解锁），直到上层接收前不会解锁
2. 上层应用同时受到一个客户端请求，获取了`kv.mu`锁， 上层应用处理客户端请求时调用`Raft.Start`，该函数尝试获取`rf.mu`，但该锁被1获取
3. 上层应用的apply线程在处理上一个applyMsg，尝试获取`kv.mu`锁，但该锁被2获取，在处理完该applyMsg之前不会接收snapshot，这时1就一直处于阻塞状态

这样的循环等待导致了死锁（之前apply能不触发这个死锁是因为我用了一个tricky的方式，在管道发送前解锁，然后在完成后上锁）

这些问题指向了一个最终解法，先将所有待发送的applyMsg在持有锁的状态下放入一个队列，然后由该队列在无锁条件下进行发送

原本也想使用带缓冲的管道作为队列，但是管道有限的大小和自带的阻塞属性太过危险，所以就自己实现了一个队列。底层用的是slice，当然用list或者直接用官方库才是正解，不过如果对效率不怎么在意的话就先用slice做一下可行性了。

```go
type ApplyQueue struct {
	queue []ApplyMsg
	mu    sync.Mutex
}

func (aq *ApplyQueue) Enqueue(val ApplyMsg) {
	aq.mu.Lock()
	defer aq.mu.Unlock()

	aq.queue = append(([]ApplyMsg)(aq.queue), val)
}

func (aq *ApplyQueue) Dequeue() (ApplyMsg, bool) {
	aq.mu.Lock()
	defer aq.mu.Unlock()

	if len(aq.queue) == 0 {
		return ApplyMsg{}, false
	}
	ret := aq.queue[0]
	aq.queue = aq.queue[1:]
	return ret, true
}

func (aq *ApplyQueue) Front() ApplyMsg {
	aq.mu.Lock()
	defer aq.mu.Unlock()

	return aq.queue[0]
}

func (aq *ApplyQueue) Size() int {
	aq.mu.Lock()
	defer aq.mu.Unlock()

	return len(aq.queue)
}

func (aq *ApplyQueue) Clean() {
	aq.mu.Lock()
	defer aq.mu.Unlock()

	aq.queue = make([]ApplyMsg, 0)
}
```

这个Dequeue一开始也想做成C++标准库那样不带副作用，不过那个不太符合我的需求，现在这个实现会返回一个bool值来表示是否操作成功，这样这整个操作就是原子的，只有yes or no两种情况。如果不返回ok的话，就无法处理在一些并发情况下可能会出现的：前面的size检查显示大于0，但是到dequeue时因为调度问题size已经变成0了。

go提倡的“do not comunicate by sharing memory; instead, share memory by communicating.” 虽然真的很酷，但要用好管道对于我这种半吊子就学过一个下午go的人来说真的很困难。管道自带的阻塞特性感觉总是成为我不想碰它的原因。因此我的实现一股子C风格，几乎没有使用像select这样的高级功能。

## 将以计时器为主的轮询改为使用条件变量

在原始的Raft的实现中，只有对同步日志的请求是使用条件变量来控制的，而其他的过程，比如commit检查，apply检查等都通过定期的轮询（基本是10ms）来检查状态是否变化。这样做整个系统也能work，不过10ms的定期轮询看起来可优化空间很大（实际上之前甚至是30ms），如果有些变化能够被负责的线程马上察觉到，想必效率会有很高的提升。

降低轮询的等待时间是一个思路，不过更正确的想法是使用条件变量

注意到这些长期运行在后台检查状态变化然后做出相应行为的线程有一定的关联性。

比如日志完成同步后进行commit check，commit check完后进行apply check，apply check完之后进行实际的发送（前面提到的无锁队列，显然也是在条件等待）

条件变量正是被设计为用来应对这种情况的。

为此我们引入这些条件变量：

```go
// Cond
// To avoid livelock, signal these conditon variables when heartbeat and Kill()
syncCond    sync.Cond // signal every time log changes
commitCond  sync.Cond // signal every time matchIndex changes
enqueueCond sync.Cond // signal every time commitIndex changes
applyCond   sync.Cond // signal every time applyQueue changes
```

每一个条件变量都绑定到raft的大锁上，在一个操作的结束时对下一个cond调用signal，具体的时机如同注释所说。

为了避免某一个cond因为某些原因丢失了，我还在每次heartbeat时主动调用所有条件变量的signal，在raft被kill时也对所有的条件变量发送signal，以此让在等待的线程能够注意到raft已经死亡从而主动退出线程

算是个人认为设计的比较好的一部分（虽然其实hints中提示了可以使用条件变量）