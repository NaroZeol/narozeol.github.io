---
date: 2024-10-19
title: "MIT-6.5840-Raft"
category: 学习经验
tags: [MIT-6.5840,分布式系统]
excerpt: "Raft论文解读以及lab3实现思路"
---


# Raft论文解读

从中央集权（单master）到自由民主的伟大过渡！（虽然选举后仍然是中央集权，而且集权程度比GFS还严重😂😂😂

TODO!!!

# Lab3实现思路

Raft虽然宣称是一个**understandable**的算法，论文阅读起来idea确实很简单，无非就是投票嘛，但实际实现起来却要面对无限的细节问题，实现起来极其麻烦，再次验证了*Talk is cheap*的不变真理

虽然从开始到第一次通过所有的测试只用了大概一个星期，但是在多次测试下证明那个实现是有问题的，有着很高的错误率

后面又花了一个星期才修完了所有的问题，这个过程非常痛苦，因为像Raft这样的并发程序几乎不可能使用调试器来进行调试，只能检查运行日志来排错。

会浪费一个星期在纠错上还有一个原因是测试的耗时比想象中多，我的实现在优化后虽然无法达到guidence里推荐的的360s内完成，但总体耗时也在380s左右，但仍然不是一个小数字，如果你想要测试上千上万次的话。虽然MIT的助教也提供了一个不错的并行化测试脚本，但是一次开启的测试数量也不能太多，太多会导致过多的时间花费在上下文切换中，导致一些对运行时间有严格限制的测试失败（3B，3C）。在我的机器上（WSL2，11th Gen Intel(R) Core(TM) i7-11600H @ 2.90GHz），单次最多并行运行80个测试。运行1000次测试要花费一个小时左右，而我们学校晚上还会断电，后续虽然把测试移动到云服务器上，但是从阿里云白嫖的2核服务器只能并行30个测试，要两天才能搞定10000次测试。

之所以要这么多测试是因为有些bug的出现是十分罕见的，有些要在三四千次的测试中才会出现一两次，不过好在因为有比较良好的运行记录，能很快定位到问题所在。

在第一次通过所有测试之后其实已经写了一篇实现思路记录，没有想到经过一个星期的修正后新的实现已经和第一次通过所有测试时相去甚远，所以只能重新写作一篇来记录一下正确的实现方案。（在新写作到这里时云服务器上的测试已经进行了5000次而无错误，但愿能完成10000次测试）

![](/assets/images/2024-10-19-raft/Figure1.png)

碎碎念到此结束，接下来回顾下实现思路，就按照lab的各阶段设置来讲吧

## Part 3A: leader election

在动手之前一定要将论文的Figure2彻底理解，事实证明很多错误都源自于没有熟悉这张图

![](/assets/images/2024-10-19-raft/Figure2.png)

在实现的时候就直接一把大锁保平安了，虽然这样做效率很差，但比起协调各种锁来说还是可以接受的

### 超时计时器

首先要实现超时计时器来启动选举流程，即`Raft.ticker`

超时计时器定期检查距上一次心跳时间是否超过了选举超时时间，如果超过了则启动选举，没有超过就按照论文描述的睡眠一个随机的时间。

为了方便配置，我将这些涉及到时间的参数都设置为一个全局常量来便于调整

```go
package raft

import "time"

const (
    TM_HeartBeatInterval time.Duration = 100 * time.Millisecond // heart beat interval
    TM_ElectionTimeout   time.Duration = 300 * time.Millisecond // election timeout
    TM_RandomWaitingTime time.Duration = 800 * time.Millisecond // randomization, wait for 0~TM_RandomWaitingTime
    MAX_RETRY_TIMES      int           = 3                      // Max times a server can try during sending a RPC
)
```

如上图所示，我的心跳周期为100ms，选举超时为300ms，而随机等待的时间在0~800ms之间

超时计时器的代码使用类似下面的方式
```go
    for !rf.killed() {
        rf.mu.Lock()

        // Heart beat timeout
        if rf.state != RS_Leader && time.Since(rf.lastHeartBeat) > TM_ElectionTimeout {
            //election
        }

        rf.mu.Unlock()
        // pause for a random amount of time
        // milliseconds.
        ms := (rand.Int63() % int64(TM_RandomWaitingTime))
        time.Sleep(time.Duration(ms))
    }
```

### 启动选举

我选择将选举在一个新的线程中进行，因为这样更方便失败时取消选举

取消选举的方式最开始的实现是在选举线程中进行各种状态检查，但后面我意识到在选举线程之外进行检查，然后使用一些手段通知选举线程取消的做法会更好一点

通知取消的方法我选择使用cancelToken来实现，虽然也可以用context包来做，但是使用原子变量来实现cancelToken会更轻量一点，因为我只有一个目的

实现的部分代码如下，后续还有很多地方我会用到相同的机制来实现任务的取消

```go
    cancelToken := int32(0)
    rf.lastHeartBeat = time.Now() // reset election timer

    go rf.election(&cancelToken)
    for {
        rf.mu.Lock()
        if time.Since(rf.lastHeartBeat) > TM_ElectionTimeout || atomic.LoadInt32(&cancelToken) == 1 ||
            rf.state != RS_Candiate || rf.killed() {
            atomic.StoreInt32(&cancelToken, 1)
            break
        }
        rf.mu.Unlock()
        time.Sleep(30 * time.Millisecond)
    }
```

在`Raft.election`中，就可以简单地通过原子操作读取cancelToken的值来确定是否要退出选举，在效率上也比加锁再检查要好一点

### 选举

到了紧张刺激的选举环节

按照Figure 2的指导来就行

首先先自增任期号，然后将`Raft.votedFor`设置为自己的编号，接着向其他的服务器发送RequestVote RPC，最后根据返回情况来确定是否能够成为Leader即可

逻辑上就这么多要做的，实际的复杂度都在接收到RequestVote RPC的处理上，不过有几个小细节可以说一下

RequestVote的发送应该要是多线程的，通过启动协程来发送请求，防止某一个线路阻塞而系统阻塞。测试框架似乎是会随机地让一些RPC无法正常送达，所以要有重试机制。但是切记不能无限重试（虽然论文中说是infinitely），这样会导致系统创建了太多的线程，导致某些烦人的超时问题。我的做法是设置固定的重试次数，设置为3

在统计票数时我直接使用一个计数器，然后在RPC返回确认投票的信息后原子地加1。这样的做法根据论文的说法其实是错误的。因为单纯的计数损失了具体是哪台机器投票的信息，这在后续如果要实现机器的增减时会有很严重的问题，不过好在整个lab3不要求实现集群配置的更改，所以在这里使用单纯的计数是没有问题的，不过还是要提一嘴这是有问题的做法，从长期来看的话

最后别忘了加上使用cancelToken来确定选举是否要取消的检查

### 处理RequestVote

接着才是这一部分最为重要的地方

不过还是按照Figure 2的指导来做即可

首先进行检查，看是否发送者的任期号比接收者的任期号低，如果低的话直接返回拒绝的回复

接着检查发送者的任期号是否高于接收者的任期号，如果高于则说明当前接收者的状态是**过时的**，是应该重置的。这时接收者将自己的任期号更新至与发送者相同，然后将`Raft.votedFor`指向nil，同时将身份转向follower
```go
    if args.Term < rf.currentTerm {
        ...
    } else if args.Term > rf.currentTerm {
        rf.currentTerm = args.Term
        rf.votedFor = nil
        rf.state = RS_Follower
        rf.persist()

        log.Printf("[%v] receive a RequestVote with higher term, change its term to %v\n", rf.me, rf.currentTerm)
    }
```

一个很难理解的地方可能是为什么将`Raft.votedFor`设置为nil而不是直接投票给发送者。这是因为接收到来自更高任期的RequestVote RPC并不意味着发送者真的是up-to-date的，一台被网络分区的机器可能会自己启动多轮选举，然后将任期号提升到一个非常高的数字，在它重新加入集群时，其向其他服务器发送的RequestVote RPC就有着很高的任期号但很显然大家都不该为它投票，因为它的日志不是up-to-date的

还有一个问题是为什么发送者和接收者任期号相同时不需要重置`Raft.votedFor`，这是因为我们希望一台机器在一个任期内只进行一次投票，所以`Raft.votedFor`是和`Raft.currentTerm`严格绑定的，只有在任期号更新是我们才会重置`Raft.votedFor`（启动选举时其实也是这样，先自增并设nil，然后再投票给自己，只是逻辑上等价于自增然后设置votedFor为自己）。换句话说只要`Raft.votedFor`从nil更变为其他值，在该任期内它就不该再被更改

再接下来就是检查日志是否发送方的日志是否up-to-date，这点和Figure 2说的一样做即可

```go
isUpToDate := (args.LastLogTerm > lastLogTerm) || (args.LastLogTerm == lastLogTerm && args.LastLogIndex >= lastLogIndex)
```

只要`Raft.votedFor`为**nil或者等于接收者的ID**（为了修正返回消息丢失的情况），并且满足up-to-date的条件，就可以把票投给接收者，返回确认的回复，同时刷新心跳计时，因为这是一个有效的响应

### 成为Leader后

这节有点涉及到后面几个部分的内容，因为我将心跳包发送和同步日志合并在了一起

在成为leader后，立刻启动一个协程，称为`Raft.serveAsLeader()`，顾名思义就是启动leader相关的工作

它的代码其实很短，因为具体的工作分散到了另外的线程上
```go
func (rf *Raft) heartBeat(cancelToken *int32) {
	for {
		if atomic.LoadInt32(cancelToken) == 1 {
			return
		}
		rf.cond.Signal()
		time.Sleep(TM_HeartBeatInterval)
	}
}

func (rf *Raft) serveAsLeader(term int) {
	cancelToken := int32(0)

	go rf.commitCheck(&cancelToken)
	go rf.heartBeat(&cancelToken)

	for {
		rf.mu.Lock()
		rf.cond.Wait()

		if atomic.LoadInt32(&cancelToken) == 1 || rf.killed() || rf.state != RS_Leader || rf.currentTerm != term {
			atomic.StoreInt32(&cancelToken, 1)
			log.Printf("[%v] lose it's power in term %v or dead, cancel all leader tasks", rf.me, term)
			rf.mu.Unlock()
			return
		}

		go rf.syncEntries(&cancelToken)
		rf.mu.Unlock()
	}
}
```

可以看到我使用了和前面讨论过的选举一样的设计方案，通过cancelToken来取消所有的相关工作，然后在主线程内定期检查状态的变化来确定是否要退出

不过有一点不同，我使用了条件变量（绑定到大锁上）和一个单独的心跳产生线程来实现定期的检查和发送AppendEntries RPC（`Raft.syncEntries()`就是用于同步日志的，理想状态下，通过该函数能让其他机器与leader的日志完全一致，不过对于现在这部分来说，因为还没有日志被添加，所以发送的总是空的RPC，也就是心跳包）

这样做的目的是为了后续的实现日志追加的时候，能够通过触发条件变量的Signal操作**立刻启动**一次同步而无需等待下一次心跳的产生，这能大幅提高系统效率

至于AppendEntries RPC的接收方一侧的做法，这里就不写了，因为在同步状态时大体上是和RequestVote RPC的接收方一样

还有个小细节就是要让日志初始时有一条空日志，因为Raft为了实现方便，下标总是从1开始。初始时有一条空日志可以保证在开始时计算是否up-to-date的时候有日志可以参考

## Part 3B: log