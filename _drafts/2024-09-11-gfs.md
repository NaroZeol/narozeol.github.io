---
date: 2024-09-11
title: "MIT-6.5840-GFS"
category: 学习经验
tags: [MIT-6.5840,分布式系统]
excerpt: "GFS论文笔记"
---

# 前言

GFS（Google File System）是google三驾马车中最为重要的一部分。在GFS之上google构建了伟大的事业，其含金量毋庸置疑，值得写一篇笔记记录一下个人的理解（后面的论文如果不是非常重要应该不会写了，写一篇总结太浪费时间了）

虽然GFS很成功，但是在从当时（2003年）的角度来看，GFS并没有提出很多创新点。它所采用的各种机制，比如多副本，单master，日志，分块等都是在过去的几十年内人们讨论分布式文件系统时经常讨论的话题。

GFS最大的优势就是它是**完全出于google自身需求构建**的一个文件系统，并且用成功运行在上千台机器上的实际成果说服了学术界，让学术界重新考虑新时代的分布式文件系统的形式。

接下来的内容说是对论文的解读，其实是把我个人在阅读GFS时所做的一些笔记摘录下来，其中不乏一些因个人理解能力不足所带来的一些错误观点。

# 设计总览

## 设计需求

正如前言所说，GFS是完全出于google自身需求构建的。GFS始终是一个仅使用于google内部的文件系统（经过有根据该论文的开源实现），所以一切的机制，接口等都首先考虑google的工作负载。

1. **适用于廉价硬件**

    GFS需要运行在由廉价的机器组成的序列上，这才能保证整个系统是容易升级和修复的，同时也让系统能够激进地淘汰可能出现故障的机器。所以GFS的设计必须假定所有的设备是不可靠的，随时会崩溃的

2. **主要支持大文件存储**

    这就是所谓的出于google自身需求构建。在新世纪以来，在摩尔定律的作用下硬件逐渐变得高效且廉价。google认为索引整个互联网是可能的（来自jyy的介绍），因此google需要一个能够存储大规模数据的文件系统，以此来支撑后续要构建在其上的应用。所以GFS面向的是大文件（一个chunk足足有64MB，就算现在看来也是很激进的分块）

3. **主要提供大规模的流式读取**

    google的工程师们观察到在构建大规模数据处理时，数据的访问通常是流式读取的。因此在GFS的设计中也假设用户读取数据时是按顺序访问的（master会在发送一个chunk的位置时顺便发送后续几个chunk的位置）

4. **文件很少更改**

    也是来自于google的需求，他们将要构建的应用是往往是用于处理静态文档的（主要是HTML文档）。这样的文档一经爬取就不再更改，因此GFS没有对随机写入做任何的优化，而是提供了record append这样更注重于追加的方案

5. **在高并发访问下保持语义一致性**

    分布式系统普遍会遇到的问题。比如对同一文件进行追加（append），因为传统的append是向用户认为的文件结尾进行写入，在并发下就会造成后执行的append覆盖了先执行的append。所以GFS提供了record append这样的方案

6. **比起低延迟更注重高带宽**

    对于google的应用程序而言，以高速率批量处理数据的能力比起快速响应更加重要。所以这让GFS的设计可以不太在意响应时间，而是更注重提高数据传输的带宽

## 提供的接口

GFS提供和Unix很像的操作接口，尽管并不是完全的符合POSIX标准。

提供以下几个标准的Unix接口，*create*, *delete*, *open*, *close*, *read*, *write*

除去这几个通用接口，GFS提供两个重要的自定义接口*snapshot*和*record append*

*snapshot*：通过GFS内部的机制进行文件的复制，在复制期间通过锁来保护复制过程的安全

*record append*：对一个文件进行追加。保证完成**至少一次**的完整追加，追加位置由GFS来决定，但保证来自同一用户的两次追加在文件中有明显的先后关系。**GFS最重要的机制之一**，GFS通过提供这个弱一致性的操作展现了巨大的性能优势，给当时追求强一致性的学院派们好好地上了一课

## 结构设计

GFS采用单一master，多chunkserver的模式

每一个程序作为一个普通的linux应用程序运行在廉价机器上。因为linux允许多任务，所以每一个chunkserver也可以运行一些其他客户端程序。这就是MapReduce那篇论文中提到的一个方案：运行mapreduce的worker的机器上同时也运行着GFS的一部分，这样mapreduce的master可以将一些需要存储在GFS的数据的工作直接分配给存储着这些数据的机器上的worker,以此实现本地访问

每一块在被创建时都分配一个全局唯一的永不可变的handle（其实就是ID）。handle是一个64bit的数字（不知道后面他们是否反悔弄小了，不过扩展起来应该不麻烦）

由master来维护所有文件系统的元数据，主要包括**名称空间**（即目录树），**访问权限**，**文件名到chunk handle集合的映射**，**chunk现在的所在位置**

所有的应用的请求会通过GFS的客户端转发，这样的工作模式类似于数据库应用。因此GFS不需要挂载在Linux的文件树上。GFS的客户端通过与master通信获取元数据，通过与特定的chunkserver通信来获取数据

所有的文件数据不会被客户端或者chunkserver缓存。这顺应了GFS设计需求中的第3点，文件总是流式顺序读取，几乎不回读，因此没有必要使用缓存。这也消除了传统的分布式文件系统往往要解决的缓存一致性问题。不过为了减小通信开销，master提供的元数据还是会被客户端缓存一段时间的。

## 单一master模型

单一master模型能为实现带来很多好处，至少单一master来管理全局资源在实现上就可以免去那些繁琐的同步机制。

但是显而易见的后果就是单一master很容易成为性能瓶颈。更严重的是因为GFS的所有的数据都是in memory的，随着系统投入使用多年，越来越多的数据使得google不得不增加master的内存大小，然而一台机器能够加量的内存大小也是有上限的。这也是为什么GFS这么快被[Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system)替代的原因之一

OK，抛开事后诸葛亮不谈，单一master至少在当时是一个正确的选择。

客户端所有的操作都要经过master的，包括访问。不过针对访问，在一次完整的访问之后，客户端在一定时间内可以通过缓存的元数据来直接访问chunkserver。

如下图所示，client先以文件名和chunk index请求master，master以chunk handle和chunk所在的位置回应。随后client根据这些信息向chunkserver发起请求，chunkserver以以数据回应。

![](/assets/images/2024-09-11-gfs/Figure1.png)

一个让人迷糊的地方可能是client如何知道他要访问的chunk是该文件的哪一个。别忘了GFS构建的前提：**几乎所有的文件都是顺序访问**。因此应用程序总是从第一个chunk开始读取数据，按顺序接着就是第二个

## chunk的大小配置

chunk的大小是一个能显著影响工作效率的参数。GFS将其设置为了一个现在看来也十分激进的数值，足足64MB

不过GFS使用了惰性分配的机制来保障磁盘使用率，所有的chunk在chunk server上都按照handle的数值来命名，作为一个普通的linux文件来保存。借用linux本身就有的一些机制，chunk虽然逻辑上是64MB，但是在实际填充满64MB之前，并不会占满64MB，只有在真正需要增加数据时才增长。

64MB的大块能够带来很多好处，首先对于流式读取的任务来说，在一段时间内总是访问相同的块，因此就能利用好缓存在客户端的元数据。其次大块能减小需要存储在master中的元数据的大小，让所有的元数据都能存储在内存中随时准备被读取，最后由于惰性分配空间，实际上大块并不会造成很大的内部碎片

不过接下来提到了一个大块的一个缺点，如果一个文件十分的小，那么在文件系统中只会占用一个块，这时如果多个客户端访问这个文件，那么这个块很容易成为热点（而其他的大文件因为有分块，一般不会导致一个块成为热点）

这个问题在google尝试通过GFS分发一个二进制可执行文件时出现，当几乎所有的机器都在尝试访问同一个文件，就算一个chunk有多个备份（通常只有三个）也难以承担这么高频的访问。对于这个问题，google的解决方案是为每一个文件设置复制因子，因子越高这个文件的chunk的备份次数就越高

## 元数据

元数据由master管理和分发。其中名称空间和file-to-chunk的映射需要持久化的存储，这样的持久化是通过日志系统实现的。

但是chunk的具体位置是不需要持久化的，这一信息通过master定期与chunkserver的通信来获取

所有的元数据在master启动之后都被置于内存之中，尽管这听起来是一个不怎么好的主意（这也是单master很快就被淘汰的原因之一），但是这确实让master所有有关元数据的操作都变得很快。

### 内存中的数据结构

将所有的数据都放置于内存中，能运行master在后台启动一些额外服务。比如定期的扫描这些数据可以实现垃圾回收，重新建立副本，处理chunkserver的故障，重新平衡chunk的放置情况

论文中也承认了将所有的数据放置于内存中的不足，但作者认为这样的不足可以直接通过增大内存来避免，然而内存也是有上限的

### chunk的位置信息

正如前面提到的，GFS的设计前提是假定所有的硬件是十分不可靠的。
因为每次启动master时，都可能有chunkserver处于不可用状态。持久化地存储位置信息是没有用的，因为今天能够访问的chunkserver明天不一定能够使用

master通过定期与chunkserver通信来获取chunk的具体存储位置，这样的设计虽然有一定的维护开销，但是显然减少了为了同步持久化存储在master和chunkserver之间的信息的努力

### 操作日志

几乎所有的现代文件系统都使用日志来保障数据的持久化存储，GFS也不例外。

日志系统只有在保证元数据的变化情况已经在系统中持久化存储之后才会将其暴露给客户端

日志也有多重备份以供恢复使用。当master崩溃后，可以通过重放日志的方式重建master。

为了尽可能减小日志的空间占用，master会定期地创建自身状态的检查点（如同很多数据库应用会做的那样）。当一个检查点被创建后，之前的日志就可以丢弃了。接下来的运行过程中如果还出现了崩溃，就可以使用检查点和自从该检查点创建以来的日志进行恢复

定期创建检查点可能比较费时间，论文中介绍了一种能够不拖延当前操作的方案：创建一个新的日志文件，对旧检查点重放旧日志即可创建当前的检查点。这一过程可以在一个新的线程进行，同时因为新到的请求不会干涉旧日志文件，因此不会造成系统的拖累

## 一致性模型

纯学术定义，看的最痛苦的一章。。。该章最好在阅读完了Record Append的具体实现的情况下再来读，不然会非常痛苦

总之就是GFS采用了弱一致性模型，能够在满足应用程序需求的情况下提高性能

### GFS的保证

所有对于命名空间（即文件树）的改动都是**原子的**，即所有的创建，重命名，删除等等会对文件树造成影响的改动都是原子的。这条保证是MapReduce这样的应用实现并发处理reduce工作的基础，所有reduce节点最后的输出会保存在GFS中，它们首先以一个临时文件名存在，然后在完成后进行重命名，在有多个相同的reduce节点时，这样的处理能保证两个reduce节点不会互相干扰

接下来的部分给出两个非常绕的定义

- **consistent**（一致性）：
    
    指所有的用户端在访问同一个区域的数据时，无论从任何块服务器上获取的，都能保证一样。即所有的块服务器在该文件的这个区域存储着**相同的副本**

- **defined**（定义性）：

    指**在保障一致性的基础上**，客户端在写入完成后紧接着读取写入的数据，总能够在数据区域中找到它写入的那部分

    假如是串行的操作，write后马上读取，没有人修改，当然是defined的

    如果是并行的操作，write后读取可能会因为别的客户端修改了写入的数据，从而导致写入者再也无法读取到自己写入的那部分

| |Write | Record Append|
| - | - | - |
| Serial success |  defined | defined |
| Concurrent successes | consistent but undefined | defined, interspersed within consistent |
| Failure | inconsistent | inconsistent |

consistent还是比较好理解的，如果有一个副本进行写入时发生了失败，那么很显然会导致该副本的某个区域和其他副本不一致。反之如果所有的副本都在系统的协调下完成写入并返回了成功的消息，那么所有的副本就都是一致的。

一个典型的Undefined的场景就是如果多个客户端对同一文件进行传统的追加。传统的追加本质上是对客户端认为的该文件结尾进行写入。因此就很容易出现两个客户端都发起了类似于WriteAt(file, offset, content...)这样的请求，**相当于在同一个地方写入两次**。这时对于**后写入的用户**而言，如果它再请求看一下该文件的结尾，它能发现结尾处有它写入的内容，所以对于该用户而言，这个文件是*defined*；但是对于**先写入的用户**而言，因为其写入被后写入的用户的请求覆盖，当它再次尝试去看该文件的结尾，它根本找不到自己写入过的痕迹，因此这次写入对于它而言就是*Undefined*
    
而Record Append因为其至少写入一次的保障，因此对于所有它的用户而言，只要服务器返回了成功的响应，它总是defined。（但是因为Record Append的一些特殊机制，在部分副本处理写入时失败，它会导致部分区域处于不一致的状态）

总之GFS主推的文件修改方式Record Append在并发下提供了一种神奇的弱一致性模型，如下图，在有副本更新失败的情况下就会导致对应的区域不一致，但是它却最终能达成defined状态

```
| CS 1 | CS 2 | CS 3 | chunk server

| ...  | ...  | ...  |
| B    | B    | B    | 一次成功的Record Append，所有CS一致
| A    | A    | X    | CS 3更新失败，导致这次的Record Append失败，该区域成为不一致的区域
| C    | C    | C    | 一次成功的Record Append，所有CS一致
| A    | A    | A    | 第一次失败的Record Append的重做，所有CS一致
```

### 对应用的影响

因为弱一致性的问题，构建在GFS上的应用需要一些特殊的调整才能正常使用。

还是回到最开始的假设，构建在GFS上的应用应该总是使用record append而不是任何的随机写入。

同时因为弱一致性可能会导致一些文件区域是属于**重复写入**，**未完整写入**，**空白**等情况（比如上面的表中的第二次追加，CS 1,2都重复写入了一次A记录，而CS 3在第二次追加时可能写入了无效的数据或者根本没写入）

因此应用需要在写入数据和取出数据时要添加一些额外信息来避开这些不一致的区域。

首先是**使用校验和**，因为有可能造成不完全的写入，所以可以在写入时顺便记录一下校验和，在读出时只需比对一下就可以知道是否是一条有效的数据。比如还是上图，如果有一个客户端在顺序地从CS 3读取数据，在读到第二条时，因为这是一次失败的写入，所以校验和的计算将是错误的，这时应用程序就可以忽略这一条数据

接着是为每一条记录做好**序号标记**。在读取时就能通过判断该序号的数据是否被读取过来过滤掉重复写入的数据。还是上图，如果一个应用从CS 1读取数据，在第一次读取第二条数据时正确读取了记录A，接着遇到第四条数据，因为这是来自同一个Record Append的数据，所以序号一致，客户端知道这条已经被读取了，就可以跳过该条数据

google内部在使用GFS时会使用一个专用的库，这个库做好了以上的两个处理，应用程序只需要处理好读写即可

# 系统交互

该部分主要讨论了client，master，chunkserver之间是怎么样交互来实现*数据修改*，*原子化的record append*以及*snapshot*

